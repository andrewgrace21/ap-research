{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU6zlFtHl57XAYVWKSlUCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewgrace21/ap-research/blob/main/APResearchPublishCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSD062mU9jMe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.sparse.linalg import cg\n",
        "import random, math, itertools\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# Authenticate and set up Google Sheets API\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "\n",
        "# -------------------- Global Parameters --------------------\n",
        "# Hyperparameter grid: [update_rule, step_size_initial, step_size_final, outer_step_size, polynomial_degree]\n",
        "params = [['', 0, 0, 0, 0]]\n",
        "sheet_names = ['']\n",
        "drive_name = ''\n",
        "\n",
        "TRIALS = 5\n",
        "ROOM_SIZE = 5  # Each room is 5x5 cells\n",
        "DEGREE = 0    # Polynomial degree for feature approximation\n",
        "EPISODES = 10000\n",
        "EVAL_INTERVAL = 100  # Evaluate policy every SAVE_EPISODE episodes (set to 100 as per methods)\n",
        "SHOW_INTERVAL = 1000  # Interval for showing plots and printing policy\n",
        "NUM_TEST = 1\n",
        "BATCH_SIZE = ROOM_SIZE  # Batch size for state-goal pairs generation\n",
        "FD_EPSILON = 1e-4\n",
        "DIVERGENCE_THRESHOLD = 50000\n",
        "DISCOUNT = 0.9\n",
        "SIZE = ROOM_SIZE * 2 + 1  # Gridworld size\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "# -------------------- Main RL Loop --------------------\n",
        "def main(update_rule, lr_init, lr_final, outer_lr):\n",
        "    epsilon = 1.0\n",
        "    step_size = lr_init\n",
        "    # Initialize weight tensor for polynomial basis functions (6 dimensions: pos(x,y), goal(x,y), distance(x,y))\n",
        "    weights = np.full((DEGREE + 1,) * 6, 0.0)\n",
        "    train_avg, test_avg, train_mse, test_mse = [], [], [], []\n",
        "    diverged = False\n",
        "\n",
        "    # Loop over episodes\n",
        "    for episode in range(EPISODES):\n",
        "        # Generate a batch of state-goal pairs from the gridworld environment\n",
        "        state_goal_pairs = generate_state_goal_pairs(update_rule)\n",
        "        td_adapted = weights.copy()\n",
        "        gradients = [[], [], [], []]\n",
        "        # Decay the outer learning rate over episodes\n",
        "        current_outer_lr = outer_lr / (1 + EPISODES / 100)\n",
        "        saved_features = ([], [])\n",
        "\n",
        "        for idx, (state, goal) in enumerate(state_goal_pairs):\n",
        "            dist = np.subtract(goal, state)\n",
        "            # Select an action using epsilon-greedy policy\n",
        "            angle = choose_action(state, goal, epsilon, weights)\n",
        "            action = (round(math.cos(angle)), round(math.sin(angle)))\n",
        "            new_state = move(state, action)\n",
        "            new_dist = np.subtract(goal, new_state)\n",
        "\n",
        "            # Calculate current and next features (polynomial basis)\n",
        "            feat = compute_features(state, goal, dist)\n",
        "            next_feat = compute_features(new_state, goal, new_dist)\n",
        "            saved_features[0].append(feat)\n",
        "            saved_features[1].append(next_feat)\n",
        "\n",
        "            reward = get_reward(state, new_state, goal)\n",
        "            old_val = np.sum(weights * feat)\n",
        "            new_val = np.sum(weights * next_feat)\n",
        "\n",
        "            # Inner update: update weights (or store gradient info) based on the update rule\n",
        "            weights, td_adapted, gradients = inner_update(\n",
        "                step_size, reward, new_val, old_val, feat, update_rule,\n",
        "                td_adapted, weights, episode, idx, gradients\n",
        "            )\n",
        "\n",
        "        # Outer update (for Fish and IDGM)\n",
        "        weights = outer_update(weights, current_outer_lr, td_adapted, update_rule, episode, state_goal_pairs, gradients, saved_features)\n",
        "\n",
        "        # Policy evaluation and divergence check every EVAL_INTERVAL episodes\n",
        "        if (episode + 1) % EVAL_INTERVAL == 0:\n",
        "            train_avg, train_mse = evaluate_policy(epsilon, weights, train_avg, train_mse, episode, test=False)\n",
        "            test_avg, test_mse = evaluate_policy(epsilon, weights, test_avg, test_mse, episode, test=True)\n",
        "            if max(train_avg) > DIVERGENCE_THRESHOLD or max(test_avg) > DIVERGENCE_THRESHOLD:\n",
        "                diverged = True\n",
        "                break\n",
        "\n",
        "        # Decay exploration and learning rate gradually\n",
        "        epsilon -= epsilon / ((EPISODES // 2) - 1)\n",
        "        step_size = lr_init / (1 + ((lr_init - lr_final) / (lr_final * EPISODES)) * episode)\n",
        "\n",
        "    # Final evaluation after training (or if diverged)\n",
        "    if not diverged:\n",
        "        train_avg, train_mse = evaluate_policy(epsilon, weights, train_avg, train_mse, EPISODES, test=False)\n",
        "        test_avg, test_mse = evaluate_policy(epsilon, weights, test_avg, test_mse, EPISODES, test=True)\n",
        "    else:\n",
        "        # If diverged, pad results to expected length\n",
        "        num_evals = EPISODES // EVAL_INTERVAL + 1\n",
        "        train_avg.extend([train_avg[-1]] * (num_evals - len(train_avg)))\n",
        "        test_avg.extend([test_avg[-1]] * (num_evals - len(test_avg)))\n",
        "        train_mse.extend([train_mse[-1]] * (num_evals - len(train_mse)))\n",
        "        test_mse.extend([test_mse[-1]] * (num_evals - len(test_mse)))\n",
        "\n",
        "    # Determine starting evaluation index based on last portion to display\n",
        "    start_idx = int(min(max(np.floor(len(train_avg) * (1 - 0.2)) - 1, 0), len(train_avg)))\n",
        "    return train_avg[start_idx:], test_avg[start_idx:], train_mse[start_idx:], test_mse[start_idx:], start_idx, update_rule, lr_init, outer_lr\n",
        "\n",
        "# -------------------- Evaluation Function --------------------\n",
        "def evaluate_policy(epsilon, weights, avg_store, mse_store, episode, test=True):\n",
        "    total_steps = 0\n",
        "    for _ in range(NUM_TEST):\n",
        "        if test:\n",
        "            state = (np.random.randint(ROOM_SIZE + 1, SIZE), np.random.randint(0, ROOM_SIZE))\n",
        "            goal = (np.random.randint(0, ROOM_SIZE), np.random.randint(ROOM_SIZE + 1, SIZE))\n",
        "        else:\n",
        "            state = (np.random.randint(0, ROOM_SIZE), np.random.randint(0, ROOM_SIZE))\n",
        "            goal = (np.random.randint(ROOM_SIZE + 1, SIZE), np.random.randint(ROOM_SIZE + 1, SIZE))\n",
        "        steps = 0\n",
        "        dist = np.subtract(goal, state)\n",
        "        while dist[0] != 0 and dist[1] != 0:\n",
        "            angle = choose_action(state, goal, epsilon, weights)\n",
        "            action = (round(math.cos(angle)), round(math.sin(angle)))\n",
        "            state = move(state, action)\n",
        "            dist = np.subtract(goal, state)\n",
        "            steps += 1\n",
        "        total_steps += steps\n",
        "    avg = total_steps / NUM_TEST\n",
        "    avg_store.append(avg)\n",
        "    mse = compute_mse(test, weights)\n",
        "    mse_store.append(mse)\n",
        "    if (episode + 1) % SHOW_INTERVAL == 0:\n",
        "        print((\"Test\" if test else \"Train\") + f\" Episode: {episode + 1}\\nAverage Steps: {np.round(avg, 4)}\\nGoal: {goal}\\nMSE: {mse}\\n\")\n",
        "        plot_value_function(weights, goal, avg_store, mse_store, episode, state, test)\n",
        "        print_policy(weights, goal, state)\n",
        "    return avg_store, mse_store\n",
        "\n",
        "# -------------------- Helper Functions --------------------\n",
        "def generate_state_goal_pairs(update_rule):\n",
        "    # Generate BATCH_SIZE random integers for positions and goals for each of 4 categories\n",
        "    x_coords = [[np.random.randint(low, high) for _ in range(BATCH_SIZE)]\n",
        "                for low, high in [(0, ROOM_SIZE), (ROOM_SIZE + 1, SIZE), (0, ROOM_SIZE), (ROOM_SIZE + 1, SIZE)]]\n",
        "    y_coords = [[np.random.randint(low, high) for _ in range(BATCH_SIZE)]\n",
        "                for low, high in [(0, ROOM_SIZE), (0, ROOM_SIZE), (ROOM_SIZE + 1, SIZE), (ROOM_SIZE + 1, SIZE)]]\n",
        "    goal_x = [[np.random.randint(ROOM_SIZE + 1, SIZE) for _ in range(BATCH_SIZE)] for _ in range(4)]\n",
        "    goal_y = [[np.random.randint(ROOM_SIZE + 1, SIZE) for _ in range(BATCH_SIZE)] for _ in range(4)]\n",
        "    # Flatten coordinate lists to produce state and goal tuples\n",
        "    states = [(x, y) for xs, ys in zip(x_coords, y_coords) for x, y in zip(xs, ys)]\n",
        "    goals = [(gx, gy) for gxs, gys in zip(goal_x, goal_y) for gx, gy in zip(gxs, gys)]\n",
        "    return list(zip(states, goals))\n",
        "\n",
        "def inner_update(step_size, reward, new_val, old_val, features, update_rule, td_adapted, weights, episode, idx, grad_list):\n",
        "    td_error = step_size * (reward + DISCOUNT * new_val - old_val) * features\n",
        "    if update_rule == 'td(0)' or episode <= 0:\n",
        "        weights += td_error\n",
        "    elif update_rule == 'Fish':\n",
        "        td_adapted -= td_error\n",
        "    elif update_rule == 'IDGM':\n",
        "        grad_list[idx // BATCH_SIZE].append(td_error * features)\n",
        "    return weights, td_adapted, grad_list\n",
        "\n",
        "def outer_update(weights, outer_lr, td_adapted, update_rule, episode, state_goal_pairs, gradients, saved_features):\n",
        "    if update_rule == 'Fish' and episode > 0:\n",
        "        weights += outer_lr * (td_adapted - weights)\n",
        "    elif update_rule == 'IDGM' and episode > 0:\n",
        "        all_grads = np.array([g for sublist in gradients for g in sublist])\n",
        "        if all_grads.size > 0:\n",
        "            avg_grad = np.mean(all_grads, axis=0)\n",
        "            weights -= outer_lr * avg_grad\n",
        "    return weights\n",
        "\n",
        "def compute_features(state, goal, dist):\n",
        "    # Create a polynomial basis of order (DEGREE) for 6 features normalized by grid size\n",
        "    grids = np.indices((DEGREE + 1,) * 6)\n",
        "    exponents = [grid.flatten() for grid in grids]\n",
        "    norm_factors = [state[0]/SIZE, state[1]/SIZE, goal[0]/SIZE, goal[1]/SIZE, dist[0]/SIZE, dist[1]/SIZE]\n",
        "    feature_vals = np.ones_like(exponents[0], dtype=float)\n",
        "    for exp, factor in zip(exponents, norm_factors):\n",
        "        feature_vals *= factor ** exp\n",
        "    return feature_vals.reshape((DEGREE + 1,) * 6)\n",
        "\n",
        "def choose_action(state, goal, epsilon, weights):\n",
        "    # With probability epsilon choose a random action; otherwise, select action with highest estimated value\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.choice([0, np.pi/2, np.pi, 3*np.pi/2])\n",
        "    else:\n",
        "        return (np.pi/2) * argmax_action(state, goal, weights)\n",
        "\n",
        "def argmax_action(state, goal, weights):\n",
        "    values = []\n",
        "    for i in range(4):\n",
        "        new_state = move(state, (round(math.cos(np.pi/2 * i)), round(math.sin(np.pi/2 * i))))\n",
        "        if new_state != state:\n",
        "            new_dist = np.subtract(goal, new_state)\n",
        "            value = np.sum(compute_features(new_state, goal, new_dist) * weights)\n",
        "        else:\n",
        "            value = -np.inf\n",
        "        values.append(value)\n",
        "    return np.argmax(values)\n",
        "\n",
        "def move(state, action):\n",
        "    new_state = [min(SIZE - 1, max(0, state[i] + action[i])) for i in range(2)]\n",
        "    # Prevent moving through walls (if on corridor boundaries)\n",
        "    for i in range(2):\n",
        "        if new_state[0] == ROOM_SIZE and new_state[1] not in {ROOM_SIZE // 2, 2 * ROOM_SIZE - ROOM_SIZE // 2}:\n",
        "            new_state[i] = state[i]\n",
        "    return tuple(new_state)\n",
        "\n",
        "def get_reward(state, new_state, goal):\n",
        "    # Reward +1 for reaching the goal; small negative reward if no move is made\n",
        "    if new_state == goal:\n",
        "        return 1\n",
        "    elif state == new_state:\n",
        "        return -1 / ROOM_SIZE\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def compute_mse(test, weights):\n",
        "    # Compute mean squared error between the estimated value and an optimal value over a sampled set of states\n",
        "    pos_x, pos_y = np.indices((SIZE - 1, SIZE - 1))\n",
        "    pos_x = [[x if x < ROOM_SIZE else x + 1 for x in row] for row in pos_x]\n",
        "    pos_y = [[y if y < ROOM_SIZE else y + 1 for y in row] for row in pos_y]\n",
        "    pos_list = list(np.array(pos_x).flatten()) + [ROOM_SIZE, SIZE - (ROOM_SIZE - 1) / 2 - 1, ROOM_SIZE, (ROOM_SIZE - 1) / 2]\n",
        "    pos_list_y = list(np.array(pos_y).flatten()) + [(ROOM_SIZE - 1) / 2, ROOM_SIZE, SIZE - (ROOM_SIZE - 1) / 2 - 1, ROOM_SIZE]\n",
        "    # Sample a set of states and compute MSE\n",
        "    all_states = []\n",
        "    for gx in range(ROOM_SIZE):\n",
        "        for gy in range(ROOM_SIZE):\n",
        "            all_states.append((pos_list[gx], pos_list_y[gy], gx, gy, gx - pos_list[gx], gy - pos_list_y[gy]))\n",
        "    sample_states = random.sample(all_states, int(len(all_states) / SIZE))\n",
        "    mse = np.mean([(np.sum(compute_features((s[0], s[1]), (s[2], s[3]), (s[4], s[5])) * weights) - optimal_value((s[0], s[1]), (s[2], s[3]), test)) ** 2 for s in sample_states])\n",
        "    return mse\n",
        "\n",
        "def optimal_value(state, goal, first=True, test=False):\n",
        "    # Recursive estimation of optimal value (using reward and discount) for a given state; stops when state equals goal.\n",
        "    if state == goal:\n",
        "        return get_reward(state, state, goal)\n",
        "    if not first:\n",
        "        return get_reward((-1, -1), state, goal) + DISCOUNT * optimal_value((state[0] - 1, state[1]), goal, first=False, test=test)\n",
        "    d = 0\n",
        "    if test:\n",
        "        state = (SIZE - state[0] - 1, state[1])\n",
        "        goal = (SIZE - goal[0] - 1, goal[1])\n",
        "    if state[0] < ROOM_SIZE and state[1] < ROOM_SIZE:\n",
        "        i = np.argmin([\n",
        "            np.sum(np.abs(np.subtract(state, (ROOM_SIZE - 1) / 2, ROOM_SIZE - 1))) +\n",
        "            np.sum(np.abs(np.subtract(goal, (SIZE - ROOM_SIZE, SIZE - 1 - (ROOM_SIZE - 1) / 2)))),\n",
        "            np.sum(np.abs(np.subtract(state, (ROOM_SIZE - 1, (ROOM_SIZE - 1) / 2)))) +\n",
        "            np.sum(np.abs(np.subtract(goal, (SIZE - 1 - (ROOM_SIZE - 1) / 2, SIZE - ROOM_SIZE))))\n",
        "        ])\n",
        "        d = min_distance(state, goal, index=i) + ROOM_SIZE + 3\n",
        "    elif state[0] > ROOM_SIZE and state[1] > ROOM_SIZE:\n",
        "        d = np.sum(np.abs(np.subtract(state, goal)))\n",
        "    else:\n",
        "        d = min_distance(state, (SIZE - 1 - (ROOM_SIZE - 1) / 2, SIZE - ROOM_SIZE)) + \\\n",
        "            min_distance(goal, (SIZE - 1 - (ROOM_SIZE - 1) / 2, SIZE - ROOM_SIZE))\n",
        "    return get_reward((d - 1, 0), (d, 0), (0, 0)) + DISCOUNT * optimal_value((d - 1, 0), (0, 0), first=False, test=test)\n",
        "\n",
        "def min_distance(state, goal, index=-1):\n",
        "    if index == -1:\n",
        "        return min(np.sum(np.abs(np.subtract(state, goal))),\n",
        "                   np.sum(np.abs(np.subtract(state, (goal[1], goal[0])))))\n",
        "    options = [\n",
        "        np.sum(np.abs(np.subtract(state, ((ROOM_SIZE - 1) / 2, ROOM_SIZE - 1)))) +\n",
        "        np.sum(np.abs(np.subtract(goal, (SIZE - ROOM_SIZE, SIZE - 1 - (ROOM_SIZE - 1) / 2)))),\n",
        "        np.sum(np.abs(np.subtract(state, (ROOM_SIZE - 1, (ROOM_SIZE - 1) / 2)))) +\n",
        "        np.sum(np.abs(np.subtract(goal, (SIZE - 1 - (ROOM_SIZE - 1) / 2, SIZE - ROOM_SIZE))))\n",
        "    ]\n",
        "    return options[index]\n",
        "\n",
        "def plot_value_function(weights, goal, avg_list, mse_list, episode, start_state, test):\n",
        "    # Generate a meshgrid for plotting the value function surface and error surface\n",
        "    x, y = np.meshgrid(np.arange(SIZE), np.arange(SIZE))\n",
        "    z_val = np.array([[np.sum(compute_features((i, j), goal, np.subtract(goal, (i, j))) * weights) for j in range(SIZE)] for i in range(SIZE)])\n",
        "    z_err = np.array([[(np.sum(compute_features((i, j), goal, np.subtract(goal, (i, j))) * weights) - optimal_value((i, j), goal, test=test)) ** 2 for j in range(SIZE)] for i in range(SIZE)])\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    gs = GridSpec(2, 2, width_ratios=[2, 1], height_ratios=[1, 1])\n",
        "    ax1 = fig.add_subplot(gs[0, 0], projection=\"3d\")\n",
        "    ax1.plot_surface(x, y, z_val)\n",
        "    ax1.set(title='Value Function', xlabel='X', ylabel='Y')\n",
        "    ax1.view_init(elev=30, azim=-60)\n",
        "\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(np.arange(len(avg_list)) * EVAL_INTERVAL, avg_list, color='red')\n",
        "    ax2.set(title='Policy Evaluation', ylim=[0, 1.2 * max(avg_list)])\n",
        "\n",
        "    ax3 = fig.add_subplot(gs[1, 0], projection='3d')\n",
        "    ax3.plot_surface(x, y, z_err)\n",
        "    ax3.set(title='Error Surface', xlabel='X', ylabel='Y')\n",
        "    ax3.view_init(elev=30, azim=-60)\n",
        "\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    ax4.plot(np.arange(len(mse_list)) * EVAL_INTERVAL, mse_list, color='red')\n",
        "    ax4.set(title='Mean Squared Error', ylim=[0, 1.2 * max(mse_list)])\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.25)\n",
        "    plt.show()\n",
        "\n",
        "def print_policy(weights, goal, start_state):\n",
        "    # Print a grid representation of the policy using symbols for actions\n",
        "    action_symbols = {0: \">\", 1: \"V\", 2: \"<\", 3: \"^\"}\n",
        "    for y in range(SIZE):\n",
        "        row = \"\"\n",
        "        for x in range(SIZE):\n",
        "            if (x, y) == goal:\n",
        "                row += \"O \"\n",
        "            elif (x, y) == start_state:\n",
        "                row += \"X \"\n",
        "            elif (x == ROOM_SIZE and y not in {ROOM_SIZE // 2, 2 * ROOM_SIZE - ROOM_SIZE // 2}) or \\\n",
        "                 (y == ROOM_SIZE and x not in {ROOM_SIZE // 2, 2 * ROOM_SIZE - ROOM_SIZE // 2}):\n",
        "                row += \"  \"\n",
        "            else:\n",
        "                row += action_symbols[argmax_action((x, y), goal, weights)] + \" \"\n",
        "        print(row)\n",
        "    print(\"\\n\" + \"_\" * 100)\n",
        "\n",
        "def col_letter(n):\n",
        "    # Convert a 1-indexed column number to an Excel-style column letter\n",
        "    result = ''\n",
        "    while n:\n",
        "        n, rem = divmod(n - 1, 26)\n",
        "        result = chr(65 + rem) + result\n",
        "    return result\n",
        "\n",
        "# -------------------- Data Export and Plotting --------------------\n",
        "for p_idx, param in enumerate(params):\n",
        "    train_avgs, test_avgs, train_mses, test_mses = [], [], [], []\n",
        "    start_idx, update_rule = 0, ''\n",
        "    DEGREE = param[4]\n",
        "    for trial in range(TRIALS):\n",
        "        t_avg, t_test, mse_train, mse_test, start_idx, update_rule, lr0, outer_lr = main(param[0], param[1], param[2], param[3])\n",
        "        train_avgs.append(t_avg)\n",
        "        test_avgs.append(t_test)\n",
        "        train_mses.append(mse_train)\n",
        "        test_mses.append(mse_test)\n",
        "\n",
        "    train_avgs = np.round(np.array(train_avgs), 4)\n",
        "    test_avgs = np.round(np.array(test_avgs), 4)\n",
        "    train_mses = np.round(np.array(train_mses), 4)\n",
        "    test_mses = np.round(np.array(test_mses), 4)\n",
        "\n",
        "    # Export results to Google Sheets\n",
        "    gc = gspread.authorize(creds)\n",
        "    worksheet = gc.open(drive_name).worksheet(sheet_names[p_idx])\n",
        "    worksheet.update('A1:A2', [[param[0]], ['Episode']])\n",
        "    episode_cells = worksheet.range(f\"A3:A{len(train_avgs[0]) + 2}\")\n",
        "    for i, cell in enumerate(episode_cells):\n",
        "        cell.value = EVAL_INTERVAL * i\n",
        "    worksheet.update_cells(episode_cells)\n",
        "\n",
        "    for trial in range(TRIALS):\n",
        "        start_col = col_letter(trial * 5 + 3)\n",
        "        end_col = col_letter(trial * 5 + 6)\n",
        "        header_range = f\"{start_col}1:{end_col}2\"\n",
        "        meta = [f\"Trial {trial + 1}\", f\"LR = {lr0}\", f\"Outer = {outer_lr}\", f\"Degree = {DEGREE}\", \"Train Avg\", \"Test Avg\", \"Train MSE\", \"Test MSE\"]\n",
        "        header_cells = worksheet.range(header_range)\n",
        "        for i, cell in enumerate(header_cells):\n",
        "            cell.value = meta[i]\n",
        "        worksheet.update_cells(header_cells)\n",
        "        data_range = f\"{start_col}3:{end_col}{len(train_avgs[trial]) + 2}\"\n",
        "        data_cells = worksheet.range(data_range)\n",
        "        data_pool = [train_avgs[trial], test_avgs[trial], train_mses[trial], test_mses[trial]]\n",
        "        for i, cell in enumerate(data_cells):\n",
        "            cell.value = data_pool[i % 4][i // 4]\n",
        "        worksheet.update_cells(data_cells)\n",
        "\n",
        "    # Plot overall performance\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
        "    for avgs, ax, title in zip([train_avgs, test_avgs, train_mses, test_mses], axes, ['Train Policy Eval', 'Test Policy Eval', 'Train MSE', 'Test MSE']):\n",
        "        for trial_data in avgs:\n",
        "            ax.plot(EVAL_INTERVAL * (np.arange(len(trial_data)) + start_idx), trial_data, color='black')\n",
        "        ax.set_title(title)\n",
        "        ax.set_ylim([0, 1.2 * np.max(avgs)])\n",
        "    plt.subplots_adjust(wspace=0.25)\n",
        "    plt.show()"
      ]
    }
  ]
}